{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "006142c2",
   "metadata": {},
   "source": [
    "<h3> You will need to run 'pip install pypdf' </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a74ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import glob\n",
    "import gradio as gr\n",
    "# Langchain imports\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings, OpenAIEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a08761e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectModel(model, model_name = None):\n",
    "    \"\"\"\n",
    "        This function allows you to select one of the models. The options are below.\n",
    "            • 'Ollama'\n",
    "            • 'OpenAI': requires an API key in the .env environment under the name of 'OPENAI_API_KEY'\n",
    "        This function returns two elements: the LLM and embeddings used to vectorize the documents.\n",
    "    \"\"\"\n",
    "    if model == 'Ollama':\n",
    "        try:\n",
    "            llm = ChatOllama(model=model_name)\n",
    "            embeddings = OllamaEmbeddings(model=model_name)\n",
    "        except Exception as error:\n",
    "            raise Exception(f\"Please ensure that your Ollama model name is correct. It current value is {model}. \" + error)\n",
    "    elif model == 'OpenAI':\n",
    "        try:\n",
    "            load_dotenv(override=True)\n",
    "            os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'Your API key if it is not loaded in the .env file.')\n",
    "        except Exception as error:\n",
    "            raise EnvironmentError(f\"Please ensure that the OpenAI API key has been inserted in the .env file under the name of 'OPENAI_API_KEY'. \" + error)\n",
    "        try:\n",
    "            llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "            embeddings = OpenAIEmbeddings()\n",
    "        except Exception as error:\n",
    "            raise Exception(f\"Please ensure that you have correctly imported the ChatOpenAI and OpenAIEmbeddings dependencies. \" + error)\n",
    "    \n",
    "    return llm, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19ce3832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRAGVectorStore(embeddings, vector_db_name, documents):\n",
    "    \"\"\"\n",
    "        This function will create the vector store for a RAG model.\n",
    "        You need to pass the embeddings of the model that you will use, along with the name of the vector database.\n",
    "    \"\"\"\n",
    "    if os.path.exists(vector_db_name):\n",
    "        Chroma(persist_directory=vector_db_name, embedding_function=embeddings).delete_collection()\n",
    "    \n",
    "    try:\n",
    "        return Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory=vector_db_name)\n",
    "    except Exception as e:\n",
    "        raise Exception(\"Please verify that the documents and embeddings have been passed correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58341c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPDFDocumentsFromDirectory(folder):\n",
    "    \"\"\"\n",
    "        This function creates the documents from the PDFs that will be used for the vector datastore.\n",
    "        It returns a list of documents.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "\n",
    "    try:\n",
    "        loader = DirectoryLoader(folder, glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
    "    except Exception as error:\n",
    "        raise Exception(f\"Please ensure that you have imported the DirectoryLoader and PyPDFLoader dependencies. \" + error)\n",
    "\n",
    "    documents = loader.load()\n",
    "\n",
    "    for document in documents:\n",
    "        # Add the name of the contract\n",
    "        document.metadata['contract'] = document.metadata['source'].split('/')[1].split('.')[0]\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19c0e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createChunksFromDocuments(documents, size = 300, overlap = 200):\n",
    "    try: \n",
    "        text_splitter = CharacterTextSplitter(chunk_size = size, chunk_overlap = overlap)\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "    except Exception as error:\n",
    "        raise Exception(\"Please ensure that the CharacterTextSplitter dependency has been imported. \" + error)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "253c722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setUpRAGModel(llm, vector_data_store):\n",
    "    try:\n",
    "        memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "    except Exception as error:\n",
    "        raise Exception(f\"Please ensure that you have imported the ConversationBufferMemory function. \" + error)\n",
    "    \n",
    "    try:\n",
    "        retriever = vector_data_store.as_retriever()\n",
    "    except Exception as error:\n",
    "        raise TypeError(\"Please ensure that you are passing a vector datastore. Error: \" + error)\n",
    "    \n",
    "    try:\n",
    "        return ConversationalRetrievalChain.from_llm(llm=llm, memory=memory, retriever=retriever)\n",
    "    except Exception as error:\n",
    "        raise ImportError(\"Please ensure that you have imported the ConversationalRetrievalChain class. Error: \" + error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ce98562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare variables here\n",
    "vector_db_name = 'contracts_db'\n",
    "model = 'Ollama'\n",
    "# If you are not planning on using Ollama, the functions will ignore the variable below.\n",
    "model_name = 'llama3.2:1b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a75a42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'m not familiar with a specific architectural style or design approach called \"RAG.\" It\\'s possible that it\\'s a lesser-known or emerging concept, or it could be a misnomer for an existing framework.\\n\\nCould you provide more context or information about what RAG is and what it refers to? I\\'d be happy to try and help you understand the concept better.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm, embeddings = selectModel(model, model_name)\n",
    "\n",
    "# Test\n",
    "llm.invoke(\"What do you think of the RAG architecture?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee63ae62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "# Create documents\n",
    "documents = createPDFDocumentsFromDirectory('Contracts')\n",
    "\n",
    "# Chunk the documents for the vector data store\n",
    "chunks = createChunksFromDocuments(documents)\n",
    "\n",
    "# Create the vector data store\n",
    "vector_store = createRAGVectorStore(embeddings=embeddings, vector_db_name=vector_db_name, documents=chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62b92f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the RAG-assisted LLM\n",
    "chain = setUpRAGModel(llm, vector_data_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a614851f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The SailPoint Confidential SaaS Agreement ( Exhibit B) does not explicitly state when it expires. However, based on the context and the fact that SailPoint offers various subscription terms for its services, it is likely that the agreement has a renewal term.\\n\\nAccording to the Schedule provided with the document, the Subscription Term is for 12 months, but it also mentions that it may be renewed for successive 12-month periods unless either party delivers written notice of non-renewal at least 30 days prior to the expiration of the then-current Subscription Term.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"When does the Sailpoint agreement expire?\"})['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4a8cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio function\n",
    "def chat(question, history):\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    return response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4901d226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3833c73f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LegalLLMEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
